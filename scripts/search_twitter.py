#!/usr/bin/env python3
"""
Grok Twitter Search - ä¼˜åŒ–ç‰ˆ
1. ç²¾ç®€ prompt å‡å°‘ input tokens
2. æ­£ç¡®çš„ xAI Responses API æ ¼å¼
3. è°ƒç”¨åæŠ¥å‘Š token æ¶ˆè€—
4. æ™ºèƒ½è§£æ Grok è¿”å›çš„æ¨æ–‡æ–‡æœ¬
"""

import os
import sys
import json
import argparse
import httpx
import re

# å…¨å±€å¤ç”¨ HTTP å®¢æˆ·ç«¯
_http_client = None

def get_client(proxy: str = None) -> httpx.Client:
    """è·å–æˆ–åˆå§‹åŒ–å…¨å±€ HTTP Client"""
    global _http_client
    if _http_client is None:
        _http_client = httpx.Client(proxy=proxy, timeout=httpx.Timeout(15.0, read=60.0))
    return _http_client


def parse_tweets_from_text(text: str, annotations: list) -> list:
    """ä» Grok è¿”å›çš„æ–‡æœ¬ä¸­æå–ç»“æ„åŒ–æ¨æ–‡æ•°æ®"""
    tweets = []
    
    # åŒ¹é…æ¨æ–‡æ¨¡å¼ï¼šç¼–å·. **@ç”¨æˆ·å** (æ—¥æœŸ): "å†…å®¹"
    # ç¤ºä¾‹ï¼š1. **@BitcoinJunkies** (Feb 27, 2026): "What's this pattern called?"
    pattern = r'(\d+)\.\s*\*\*@([^*]+)\*\*\s*\(([^)]+)\):\s*"([^"]+)"'
    
    matches = re.findall(pattern, text)
    
    # æ„å»º URL æ˜ å°„ï¼ˆä» annotations ä¸­æå–ï¼‰
    url_map = {}
    for ann in annotations:
        if ann.get("type") == "url_citation":
            title = ann.get("title", "")
            url = ann.get("url", "")
            if title and url:
                url_map[title] = url
    
    for idx, (num, author, date, content) in enumerate(matches):
        tweet_url = url_map.get(str(idx + 1), "")
        tweets.append({
            "author": f"@{author.strip()}",
            "content": content.strip(),
            "timestamp": date.strip(),
            "likes": 0,
            "retweets": 0,
            "url": tweet_url
        })
    
    # å¦‚æœæ²¡æœ‰æå–åˆ°ç»“æ„åŒ–æ•°æ®ï¼Œè¿”å›æ•´ä¸ªæ–‡æœ¬ä½œä¸ºæ‘˜è¦
    if not tweets and text.strip():
        tweets.append({
            "author": "Grok Summary",
            "content": text[:800] + "..." if len(text) > 800 else text,
            "timestamp": "Now",
            "likes": 0,
            "retweets": 0,
            "url": ""
        })
    
    return tweets


def search_twitter(
    query: str, 
    api_key: str, 
    api_base: str = "https://api.x.ai/v1", 
    max_results: int = 10,
    proxy: str = None,
    analyze: bool = False
) -> dict:
    """è°ƒç”¨ xAI APIï¼Œä½¿ç”¨åŸç”Ÿå·¥å…·è¿”å›æœºåˆ¶"""
    
    url = f"{api_base.rstrip('/')}/responses"
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }

    # æ¨¡å‹é€‰æ‹©ï¼šåªæœ‰ reasoning æ¨¡å‹æ”¯æŒ x_search å·¥å…·
    model = "grok-4-1-fast-reasoning"
    
    # ç²¾ç®€çš„ payloadï¼Œå‡å°‘ input tokens
    payload = {
        "model": model,
        "input": f"Search Twitter for: {query}. Return up to {max_results} tweets.",
        "tools": [{"type": "x_search"}],
        "temperature": 0.0
    }

    try:
        client = get_client(proxy)
        response = client.post(url, headers=headers, json=payload)
        response.raise_for_status()
        
        data = response.json()
        
        # åˆå§‹åŒ–ç»“æœ
        result = {
            "status": "success",
            "query": query,
            "tweets": [],
            "model_used": model,
            "usage": {},
            "cost_report": ""
        }
        
        # æå– usage ä¿¡æ¯
        usage = data.get("usage", {})
        input_tokens = usage.get("input_tokens", 0)
        output_tokens = usage.get("output_tokens", 0)
        total_tokens = usage.get("total_tokens", 0) or (input_tokens + output_tokens)
        x_search_calls = usage.get("x_search_calls", 0)
        
        result["usage"] = {
            "input_tokens": input_tokens,
            "output_tokens": output_tokens,
            "total_tokens": total_tokens,
            "x_search_calls": x_search_calls
        }
        
        # ç”Ÿæˆæˆæœ¬æŠ¥å‘Š
        input_cost = (input_tokens / 1_000_000) * 0.20
        output_cost = (output_tokens / 1_000_000) * 0.50
        total_cost = input_cost + output_cost
        
        result["cost_report"] = (
            f"ğŸ“Š Token æ¶ˆè€—æŠ¥å‘Š:\n"
            f"   Input tokens:  {input_tokens:,}\n"
            f"   Output tokens: {output_tokens:,}\n"
            f"   Total tokens:  {total_tokens:,}\n"
            f"   X Search calls: {x_search_calls}\n"
            f"   ğŸ’° é¢„ä¼°æˆæœ¬: ${total_cost:.4f} (${total_cost*1000:.2f}/åƒæ¬¡)"
        )
        
        # è§£ææ¨æ–‡æ•°æ®
        tweets = []
        output_list = data.get("output", [])
        
        for response_item in output_list:
            if not isinstance(response_item, dict):
                continue
            
            # ç­–ç•¥ 1: ä» message å†…å®¹ä¸­è§£æï¼ˆä¸»è¦æ¥æºï¼‰
            if response_item.get("type") == "message":
                message = response_item.get("message", response_item)
                content = message.get("content", "")
                
                # å¦‚æœ content æ˜¯åˆ—è¡¨ï¼ˆæ–°çš„ API æ ¼å¼ï¼‰
                if isinstance(content, list):
                    for c in content:
                        if isinstance(c, dict) and c.get("type") == "output_text":
                            text = c.get("text", "")
                            annotations = c.get("annotations", [])
                            
                            # ä»æ–‡æœ¬ä¸­æå–æ¨æ–‡
                            extracted = parse_tweets_from_text(text, annotations)
                            tweets.extend(extracted)
                
                # å¦‚æœ content æ˜¯å­—ç¬¦ä¸²ï¼ˆæ—§çš„ API æ ¼å¼ï¼‰
                elif isinstance(content, str) and content.strip():
                    tweets.append({
                        "author": "Grok Summary",
                        "content": content[:500] + "..." if len(content) > 500 else content,
                        "timestamp": "Now",
                        "likes": 0,
                        "retweets": 0,
                        "url": ""
                    })
        
        result["tweets"] = tweets[:max_results]
        
        # å°† token æ¶ˆè€—æŠ¥å‘Šæ·»åŠ åˆ°ç»“æœä¸­
        result["token_report"] = result["cost_report"]
        
        # æ‰“å°åˆ° stdout ç¡®ä¿ OpenClaw èƒ½çœ‹åˆ°
        print(result["cost_report"], flush=True)
        
        return result

    except httpx.HTTPStatusError as e:
        error_msg = f"API é”™è¯¯: {e.response.status_code} - {e.response.text[:200]}"
        print(f"âŒ {error_msg}", file=sys.stderr)
        return {"status": "error", "message": error_msg}
    except httpx.RequestError as e:
        error_msg = f"ç½‘ç»œ/ä»£ç†é”™è¯¯: {e}"
        print(f"âŒ {error_msg}", file=sys.stderr)
        return {"status": "error", "message": error_msg}
    except Exception as e:
        error_msg = f"æœªçŸ¥é”™è¯¯: {e}"
        print(f"âŒ {error_msg}", file=sys.stderr)
        return {"status": "error", "message": error_msg}


def run_interactive_mode(api_key: str, default_proxy: str):
    """çº¯æ•°å­—èœå•äº¤äº’æ¨¡å¼"""
    while True:
        print("\n" + "="*40)
        print("  ğŸ¦ Grok Twitter æœç´¢")
        print("="*40)
        print(f"å½“å‰ä»£ç†: {default_proxy or 'ç›´è¿'}")
        print("1. æé€Ÿæ£€ç´¢")
        print("2. æ·±åº¦åˆ†æ")
        print("0. é€€å‡º")
        print("="*40)
        
        try:
            choice = input("è¯·é€‰æ‹©: ").strip()
            if choice == '0':
                break
            elif choice in ('1', '2'):
                query = input("\næœç´¢å…³é”®è¯: ").strip()
                if not query:
                    continue
                
                print(f"\nğŸ” æœç´¢ä¸­...")
                res = search_twitter(
                    query=query, 
                    api_key=api_key, 
                    proxy=default_proxy, 
                    analyze=(choice == '2')
                )
                
                # æ‰“å°ç»“æœ
                output = {k: v for k, v in res.items() if k != "cost_report"}
                print(json.dumps(output, ensure_ascii=False, indent=2))
            else:
                print("[!] æ— æ•ˆè¾“å…¥")
        except KeyboardInterrupt:
            print("\nğŸ‘‹ å†è§")
            break
        except Exception as e:
            print(f"\n[!] é”™è¯¯: {e}")


def main():
    if len(sys.argv) > 1:
        parser = argparse.ArgumentParser(description="Grok Twitter Search")
        parser.add_argument("--query", required=True, help="æœç´¢æŸ¥è¯¢")
        parser.add_argument("--api-key", help="Grok API Key")
        parser.add_argument("--api-base", default="https://api.x.ai/v1")
        parser.add_argument("--max-results", type=int, default=10)
        parser.add_argument("--proxy", help="SOCKS5 ä»£ç†")
        parser.add_argument("--analyze", action="store_true", help="å¯ç”¨æ¨ç†æ¨¡å¼")
        
        args = parser.parse_args()
        
        api_key = args.api_key or os.environ.get("GROK_API_KEY")
        if not api_key:
            print(json.dumps({"status": "error", "message": "ç¼ºå°‘ API Key"}))
            sys.exit(1)
            
        proxy = args.proxy or os.environ.get("SOCKS5_PROXY")
        
        result = search_twitter(
            args.query, api_key, args.api_base, 
            args.max_results, proxy, args.analyze
        )
        
        # è¾“å‡ºç»“æœ
        output = {k: v for k, v in result.items() if k != "cost_report"}
        print(json.dumps(output, ensure_ascii=False, indent=2))
    else:
        api_key = os.environ.get("GROK_API_KEY")
        if not api_key:
            print("[!] æœªè®¾ç½® GROK_API_KEY")
            sys.exit(1)
        proxy = os.environ.get("SOCKS5_PROXY")
        run_interactive_mode(api_key, proxy)


if __name__ == "__main__":
    main()
